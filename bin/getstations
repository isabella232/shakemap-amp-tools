#!/usr/bin/env python

# stdlib imports
import argparse
import logging
import os.path
import sys
import textwrap
import warnings
from collections import namedtuple
from xml.dom import minidom
import shutil
from datetime import datetime

# third party imports
from h5py.h5py_warnings import H5pyDeprecationWarning
from datetime import datetime
import configobj
import pandas as pd

# local imports
from gmprocess.args import add_shared_args
from gmprocess.io.fetch_utils import (update_config,
                                      save_shakemap_amps, download,
                                      draw_stations_map)
from gmprocess.logging import setup_logger
from gmprocess.processing import process_streams
from gmprocess.report import build_report_latex
from gmprocess.config import get_config
from gmprocess.event import ScalarEvent
from gmprocess.plot import summary_plots
from impactutils.io.table import read_excel, dataframe_to_xml


TAG_FMT = '%Y%m%d%H%M%S'
# 2008-07-29T18:42:15Z
TIMEFMT = '%Y-%m-%dT%H:%M:%SZ'


class MyFormatter(argparse.RawTextHelpFormatter,
                  argparse.ArgumentDefaultsHelpFormatter):
    pass


def format_helptext(text):
    '''Format help text, including wrapping.
    '''
    return '\n'.join(textwrap.wrap(text))


def append_file(files_created, tag, filename):
    if tag in files_created:
        files_created[tag].append(filename)
    else:
        files_created[tag] = [filename]


def process_event(outdir, event,
                  config, input_directory,
                  logfile,
                  files_created,
                  no_plots=False):

    # create the amptools directory where supporting amptools
    # files will be found
    workname = os.path.join(outdir, 'amptools', 'workspace.hdf')

    # Need to initialize rstreams because later we check is's length
    rstreams = []

    logging.info('Downloading/loading raw streams...')
    workspace, workspace_file, rstreams = download(
        event, outdir, config, input_directory)

    # clean up raw data
    logging.info('Deleting any downloaded raw data...')
    rawdir = os.path.join(outdir, 'raw')
    for tfile in os.listdir(rawdir):
        if not tfile.endswith('.png'):
            ffile = os.path.join(rawdir, tfile)
            if os.path.isfile(ffile):
                os.remove(ffile)
            elif os.path.isdir(ffile):
                shutil.rmtree(ffile)

    append_file(files_created, 'Workspace', workname)
    process_tag = 'processed'

    logging.info('Processing raw streams for event %s...' % event.id)
    pstreams = process_streams(rstreams, event, config=config)

    if len(pstreams):

        with warnings.catch_warnings():
            warnings.simplefilter("ignore",
                                  category=H5pyDeprecationWarning)
            logging.info('Adding processed streams to workspace...')
            workspace.addStreams(event, pstreams, label=process_tag)
            logging.info('Calculating metrics...')
            workspace.calcMetrics(event.id,
                                  labels=[process_tag],
                                  config=config)

        if not pstreams.n_passed:
            print('No stations passed QA checks. XML output will not be created.')
            sys.exit(0)
            del workspace

        logging.info(
            'Creating shakemap table for event %s...' % event.id)
        shakemap_file = save_shakemap_amps(pstreams, event, outdir)
        append_file(files_created, 'shakemap', shakemap_file)

        del workspace

        # convert the spreadsheet into the station xml data format.
        df, reference = read_excel(shakemap_file)
        netid = df['NETID'].iloc[0]
        if isinstance(netid, pd.Series):
            netid = netid[netid.index[0]]
        xmlfile = os.path.abspath(os.path.join(
            outdir, '..', '%s_dat.xml' % netid))
        dataframe_to_xml(df, xmlfile, reference=reference)
        logging.info('Peak ground motions written to file %s.' % xmlfile)
        append_file(files_created, 'ShakeMap XML', xmlfile)
        logging.info('Finishing event %s' % event.id)

        if no_plots:
            return workname
        logging.info(
            'Creating diagnostic plots for event %s...' % event.id)
        plot_dir = os.path.join(outdir, 'plots')
        if not os.path.isdir(plot_dir):
            os.makedirs(plot_dir)
        for stream in pstreams:
            logging.info('Creating summary plot for %s...' % stream.get_id())
            summary_plots(stream, plot_dir, event)

            mapfile = draw_stations_map(pstreams, event, outdir)
            append_file(files_created, 'Station map', mapfile)

        logging.info(
            'Creating diagnostic report for event %s...' % event.id)
        # Build the summary report?
        build_conf = config['build_report']
        report_format = build_conf['format']
        if report_format == 'latex':
            report_file, success = build_report_latex(
                pstreams,
                outdir,
                event,
                config=config
            )
        else:
            report_file = ''
            success = False
        if os.path.isfile(report_file) and success:
            append_file(files_created, 'Summary report', report_file)

        # remove processed plots
        plotdir = os.path.join(outdir, 'plots')
        shutil.rmtree(plotdir)

    else:
        workname = None

    return workname


def find_workspace_files(outdir):
    workspace_files = []
    for root, dirs, files in os.walk(outdir):
        for tfile in files:
            if tfile.endswith('.hdf'):
                fullfile = os.path.join(root, tfile)
                workspace_files.append(fullfile)
    return workspace_files


def main(args):
    tstart = datetime.utcnow()
    # config handling
    configfile = args.config
    if configfile is not None:
        config = update_config(configfile)
        if config is None:
            print('\nCustom config file %s is invalid. Exiting.')
            sys.exit(1)

    else:
        config = get_config()

    # confirm that the eventID is a directory for ShakeMap.
    homedir = os.path.expanduser('~')
    smprofile = os.path.join(homedir, '.shakemap', 'profiles.conf')
    if not os.path.isfile(smprofile):
        print('ShakeMap is not installed on this system. Exiting.')
        sys.exit(1)
    cfg = configobj.ConfigObj(infile=smprofile)
    current_profile = cfg['profile']
    profile = cfg['profiles'][current_profile]
    data_path = profile['data_path']
    event_folder = os.path.join(data_path, args.eventid, 'current')
    if not os.path.isdir(event_folder):
        fmt = 'No ShakeMap event directory for %s exists on this system. Exiting.'
        print(fmt % args.eventid)
        sys.exit(1)

    # get the event object from the event directory
    event_file = os.path.join(event_folder, 'event.xml')
    root = minidom.parse(event_file)
    eq = root.getElementsByTagName('earthquake')[0]
    eid = eq.getAttribute('id')
    etime = datetime.strptime(eq.getAttribute('time'), TIMEFMT)
    elat = float(eq.getAttribute('lat'))
    elon = float(eq.getAttribute('lon'))
    edepth = float(eq.getAttribute('depth'))
    emag = float(eq.getAttribute('mag'))
    event = ScalarEvent()
    event.fromParams(eid, etime, elat, elon, edepth, emag)
    root.unlink()

    # if user specified an input directory, then we won't try to
    # fetch data from an online source.
    input_directory = args.directory

    workspace_files = []
    files_created = {}

    outdir = os.path.join(event_folder, 'amptools')
    if not os.path.isdir(outdir):
        os.makedirs(outdir)

    logfile = os.path.join(outdir, 'getstations.log')

    # setup logging to write to a logfile, unless user
    # specified stdout with -d

    setup_logger(args)
    if not args.debug:
        logger = logging.getLogger()
        stream_handler = logger.handlers[0]
        fhandler = logging.FileHandler(logfile)
        fhandler.setFormatter(stream_handler.formatter)
        logger.removeHandler(stream_handler)
        logger.addHandler(fhandler)

    workname = process_event(
        outdir, event,
        config, input_directory,
        logfile, files_created, no_plots=args.no_plots)
    if workname is not None:
        workspace_files.append(workname)

        logging.info('%i workspace files created' % len(workspace_files))

        print('\nThe following files have been created:')
        for file_type, file_list in files_created.items():
            print('File type: %s' % file_type)
            for fname in file_list:
                print('\t%s' % fname)
    else:
        print('No ground motion data were found. Exiting.')

    tend = datetime.utcnow()
    dt = (tend - tstart).total_seconds()
    minutes = dt // 60
    seconds = dt % 60
    fmt = '\nElapsed processing time: %i minutes, %i seconds.'
    print(fmt % (minutes, seconds))

    print('\nProcessing is complete.\n')


if __name__ == '__main__':
    description = '''Download, process, and create ground motion inputs for ShakeMap.

This program will allow the user to:
   - download raw data from a number of sources, including:
     - Any FDSN provider which serves waveform data
     - Japan's KNET/KikNet repository (requires login info)
     - GeoNet (NZ)
     - Turkey
   - process data from the sources above, plus those data from networks
     without a web interface, including:
     - Mexico (UNAM)
     - Center for Engineering Strong Motion Data (CESMD)
     - Iran
     - Taiwan Central Weather Bureau

This program will fetch data, process it, and write a ShakeMap XML data format
file in the input event directory. Additionally, in an *amptools* subdirectory,
it will write the following files/directories:
  - report.pdf A PDF LateX report on the processing performed on each waveform
  - workspace.hdf An HDF file using the ASDF protocol for storing raw and 
    processedÂ waveform data
  - amptools_data.xlsx Excel spreadsheet version of XML format data
  - png/ Directory containing PNG plots of raw waveform data.

This program assumes that the user has both the gmprocess package and ShakeMap
installed on their machine.
'''

    parser = argparse.ArgumentParser(
        description=description, formatter_class=MyFormatter)

    # ***** Required arguments
    hstr = '''Event ID matching ShakeMap data directory on this system.'''
    parser.add_argument('eventid', help='',
                        metavar="EVENTID", type=str)

    help_config = format_helptext(
        'Supply custom configuration file'
    )
    parser.add_argument(
        '--config', help=help_config, action='store',
        type=str, dest='config'
    )

    parser.add_argument(
        '--directory', help='directory containing raw data',
    )

    parser.add_argument('-n', '--no_plots',
                        help='Disable diagnostic plots.',
                        action='store_true', default=False)
    # ***** Shared arguments
    parser = add_shared_args(parser)
    pargs = parser.parse_args()
    main(pargs)
